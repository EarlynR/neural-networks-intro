{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression using Gradient Descent\n",
    "\n",
    "In the previous notebook, we looked at a high level overview of solving our simple linear regression problem using gradient descent. Now, we'll formalize that, actually calculate the derivatives that we need to implement it, and use `numpy` to do so. \n",
    "\n",
    "## Using Gradient Descent for Simple Linear Regression\n",
    "\n",
    "### Gradient Descent Procedure \n",
    "\n",
    "Formally, with gradient descent we will do the following: \n",
    "\n",
    "1. Randomly initialize values for our coefficients, \n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> and\n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\>   \n",
    "\n",
    "2. While we haven't met some stopping condition:   \n",
    " A. Calculate our predicted outcomes, \n",
    "<img src=\"../imgs/variables/yhat.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>, using our simple linear regression equation\n",
    "(<img src=\"../imgs/equations/simp_linear.png\" width=100 style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" \\>).  \n",
    " B. Calculate the error for each observation using the true values\n",
    "<img src=\"../imgs/variables/yi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=17 \\>, \n",
    "our predicted values \n",
    "<img src=\"../imgs/variables/yhati.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=16 \\>, \n",
    "and our error formula: \n",
    "<img src=\"../imgs/equations/ind_squared_error.png\" width=110 \\>      \n",
    " C. For each observation, calculate the gradient of the error with respect to each one of our coefficients (\n",
    "<img src=\"../imgs/derivatives/ei_beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>, \n",
    "<img src=\"../imgs/derivatives/ei_beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>\n",
    "), and then use the average across observations to update the coefficients: \n",
    "<img src=\"../imgs/updates/beta0_simp_linear_update.png\" width=150 \\>\n",
    "<img src=\"../imgs/updates/beta1_simp_linear_update.png\" width=150 \\>\n",
    "\n",
    "Note that we are subtracting off the gradient in step 2C because the gradient gives us the direction of steepest ascent (and we want to take the direction of steepest descent to minimize our error). Note also that \n",
    "<img src=\"../imgs/variables/alpha.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=15\\> \n",
    "in step 2C is simply the learning rate (e.g. how much of the coefficient updates actually get applied). \n",
    "\n",
    "Now, let's actually calculate the derivatives. \n",
    "\n",
    "### Derivative Calculations\n",
    "\n",
    "Recall that we'll use the chain rule to calculate the updates that we need for step 2C: \n",
    "\n",
    "<img src=\"../imgs/derivatives/ei_beta0_chain.png\" width=120\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta1_chain.png\" width=110\\>\n",
    "\n",
    "To calculate these, we'll need three quantities - \n",
    "<img src=\"../imgs/derivatives/ei_yi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"../imgs/derivatives/yhati_beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"../imgs/derivatives/yhati_beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>. We can calculate these as follows: \n",
    "\n",
    "<img src=\"../imgs/derivatives/ei_yi_soln.png\" width=275 \\>\n",
    "<img src=\"../imgs/derivatives/yhati_beta0_soln.png\" width=75 \\>\n",
    "<img src=\"../imgs/derivatives/yhati_beta1_soln.png\" width=90 \\>\n",
    "\n",
    "We can then plug these in to obtain our updates for step 2C: \n",
    "\n",
    "<img src=\"../imgs/derivatives/ei_beta0_chain_soln.png\" width=350\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta1_chain_soln.png\" width=290\\>\n",
    "\n",
    "Check out these [notes from Andrej Karpathy](http://karpathy.github.io/neuralnets/) for a refresher on gradient descent or a more thorough but still simplistic discussion of backpropagation (the code in the notes is written in `JavaScript`, but it is fairly simplistic and Andrej does an excellent job with his explanations). \n",
    "\n",
    "## Simple Linear Regression using Gradient Descent with `numpy`\n",
    "\n",
    "We'll begin our `numpy` implementation by generating some fake data. To obtain some fake data that follows a univariate linear relationship, we'll use a function from the `datasets/general.py`. With the function `gen_simple_linear`, we'll input a <img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, \n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\>, \n",
    "and number of observations. We'll receive as output data that follows a univariate linear relationship specified by \n",
    " <img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> \n",
    "and \n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\> \n",
    "(<img src=\"../imgs/equations/simp_linear.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=110 \\>). Using this data, we'll learn the coefficients using gradient descent as specified above. We'll also double check our results with the `LinearRegression` model from `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datasets.general import gen_simple_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn_w_gradient_descent(xs, ys): \n",
    "    learning_rate = 0.5\n",
    "    # Step 1 - randomly initialize values for our coefficients. \n",
    "    beta_0, beta_1 = np.random.randint(1, 10, size=2)\n",
    "    print(\"Initial gradient descent beta_0: {}\".format(beta_0))              \n",
    "    print(\"Initial gradient descent beta_1: {}\".format(beta_1))\n",
    "\n",
    "    for _ in range(5000): \n",
    "        # Step 2A - calculate our predicted outcomes. \n",
    "        yhats = beta_0 + beta_1 * xs\n",
    "        \n",
    "        # Step 2B - calculate our errors. \n",
    "        es = (ys - yhats)\n",
    "        \n",
    "        # Step 2C - calculate the gradient of the error with respect to the coefficients, \n",
    "        # and use it to update the coefficients. \n",
    "        d_beta_0 = -es\n",
    "        d_beta_1 = -es * xs\n",
    "        beta_0 -= learning_rate * d_beta_0.mean()\n",
    "        beta_1 -= learning_rate * d_beta_1.mean()\n",
    "          \n",
    "    print(\"Final gradient descent beta_0: {}\".format(beta_0))              \n",
    "    print(\"Final gradient descent beta_1: {}\\n\".format(beta_1))\n",
    "    \n",
    "def learn_w_standard_linear_regression(xs, ys): \n",
    "    \n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(xs, ys)\n",
    "    \n",
    "    print(\"Sklearn linear regression beta_0: {}\".format(linear_model.intercept_[0]))\n",
    "    print(\"Sklearn linear regression beta_1: {}\".format(linear_model.coef_[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual beta_0: 5\n",
      "Actual beta_1: 4\n",
      "\n",
      "Initial gradient descent beta_0: 5\n",
      "Initial gradient descent beta_1: 6\n",
      "Final gradient descent beta_0: 4.999999999999992\n",
      "Final gradient descent beta_1: 4.000000000000014\n",
      "\n",
      "Sklearn linear regression beta_0: 5.0\n",
      "Sklearn linear regression beta_1: 4.000000000000001\n"
     ]
    }
   ],
   "source": [
    "# Randomly generate a beta_0, beta_1, and number of observations, used to generate \n",
    "# fake data to fit. We need a minimum of 2 obs. \n",
    "true_beta_0, true_beta_1 = np.random.randint(2, 10, size=2) \n",
    "n_obs = np.random.randint(9000, 11000) \n",
    "print('Actual beta_0: {}'.format(true_beta_0))\n",
    "print('Actual beta_1: {}\\n'.format(true_beta_1))\n",
    "\n",
    "# Generate the data that follows a linear relationship specified \n",
    "# by true_beta_0 and true_beta_1.\n",
    "xs, ys = gen_simple_linear(true_beta_0, true_beta_1, n_obs)\n",
    "\n",
    "# Learn the coefficients using gradient descent and `LinearRegression` from sklearn. \n",
    "learn_w_gradient_descent(xs, ys)\n",
    "learn_w_standard_linear_regression(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above demonstrates that we can in fact solve linear regression using gradient descent, and that we do obtain the coefficients that we expect. We can even run it multiple times, and see that each time, our gradient descent procedure is able to learn the right coefficient values for \n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> \n",
    "and \n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22\\>. \n",
    "\n",
    "While this in itself isn't terribly useful, it'll help set the stage for understanding complicated neural network architectures that we'll look at in subsequent notebooks. At the end of the day, most (if not all) neural networks can be viewed as having a **forward** and **backward** propagation step where we can use some flavor of gradient descent to update and learn our coefficients (often called weights in neural network land).\n",
    "\n",
    "We'll now move on to coding this up using `theano`, a python library that allows us to define computational graphs and benefit from automatic differentiation. Libraries like this will be extremely useful when building incredibly complicated neural networks for which it is difficult and time consuming to derive the update rules by hand. \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
