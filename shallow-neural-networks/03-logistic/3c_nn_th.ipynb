{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using `theano`\n",
    "\n",
    "We'll now walk through using gradient descent to solve logistic regression with `theano`. Just as with our linear regression problems, we'll begin by building up a computational graph, and then let `theano` handle the differentiation for us. \n",
    "\n",
    "With the automatic differentiation that `theano` offers, we'll still have to specify the details of what derivatives to compute, but will not actually have to take any by hand. This feature will become invaluable as we move towards neural networks with hundreds of thousands of parameters. \n",
    "\n",
    "## Computational Graphs for Logistic Regression \n",
    "\n",
    "As we code up our forward and backward propagation steps with `theano`, let's keep their visuals around as a reference: \n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "<img src=\"../imgs/custom/logistic_comp_graph_condensed_forprop.png\" width=300\\>\n",
    "\n",
    "### Backward Propagation\n",
    "\n",
    "<img src=\"../imgs/custom/logistic_comp_graph_condensed_backprop.png\" width=400\\>\n",
    "\n",
    "### Building a computational graph with `theano`\n",
    "\n",
    "As previously mentioned, the biggest difference between our `numpy` and `theano` solutions for logistic regression is that we'll be able to use the automatic differentiation that `theano` offers. In order to do so, we'll have to tell `theano` what quantity to take the derivative of and the parameters that it should take the derivative with respect to, but after that it'll handle the rest. \n",
    "\n",
    "Let's take a look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "from datasets.general import gen_multiple_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_theano_graph(): \n",
    "    learning_rate = 0.1\n",
    "    # 1. Define placeholder matrices for inputs. \n",
    "    xs, ys = T.dmatrices('xs', 'ys') # returns one or more matrices of type `float64`\n",
    "    # 2. Define randomly initialized floats for our betas. \n",
    "    betas = theano.shared(np.random.random(size=(4, 1)), name='betas')\n",
    "\n",
    "    # 3. Define the equation that generates predictions. \n",
    "    yhats = 1 / (1 + T.numpy.exp(-T.dot(xs, betas)))\n",
    "    # 4. Define the equation that generates our errors. \n",
    "    es = -(ys * np.log(yhats) + (1 - ys) * np.log(1 - yhats))\n",
    "    # 5. Define the aggregate cost (mean of squared errors)\n",
    "    E = es.mean()\n",
    "    # 6. Take advantage of `theanos` automatic differentiation, and use the derivatives\n",
    "    #    to perform the update step. \n",
    "    d_betas = T.grad(E, betas)\n",
    "    updates = [(betas, betas - learning_rate * d_betas)]\n",
    "    # 7. Define a function that we can feed inputs to, obtain outputs from, and \n",
    "    #    perform updates on our coefficients / train them. \n",
    "    train = theano.function(inputs=[xs, ys], outputs=[es, yhats], \n",
    "                            updates=updates)\n",
    "    \n",
    "    for idx, beta in enumerate(betas.get_value()): \n",
    "        print(\"Initial theano value for beta_{}: {}\".format(idx, beta[0]))\n",
    "    print('\\n')\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our solution here for logistic regression is going to look extremely similar to our solution for multiple linear regression. The only differences will be in the calculations of the predicted values (`yhats`) and errors (`es`) in steps `3` and `4`. Let's walk through each piece of the code, though, to refresh our memory on how it's working. \n",
    "\n",
    "From a high level, `get_theano_graph` returns a `theano.function` object that performs one iteration of our gradient descent procedure. It performs both forward propagation (steps `1-5`) and backward propagation (step `6`), and in the end [generates a callable function](http://deeplearning.net/software/theano/library/compile/function.html#module-theano.compile.function) that we can later use to perform one iteration of our gradient descent procedure (step `7`).  \n",
    "\n",
    "In step `6`, we use `T.grad` to take advantage of the automatic differentiation that `theano` offers. With `T.grad`, we pass as the first argument the quantity to take the derivative of and as the second what to take the derivative with respect to. Recall that the `theano.grad` function expects a scalar input, which is why we differentiate the mean of the squared-error (as opposed to vector of individual errors). Mathematically, taking the derivative of the mean is the same as taking the mean of the individual derivatives, since the derivative of a sum is equal to the sum of the individual derivatives. \n",
    "\n",
    "The callable function generated in step `7` takes inputs (via the `inputs` argument), runs them through the graph, and returns outputs (specified by the `outputs` argument). This function has the side effect of updating the values of each of the betas in our `betas` shared variable (specified by the `updates` argument), which is ultimately how we learn the values for our coefficients. \n",
    "\n",
    "It's important to again note that our `betas` variable is defined as a [shared variable](http://deeplearning.net/software/theano/tutorial/examples.html#using-shared-variables), which tells `theano` that it should **share** the values held in this variable across calls to the function generated in step `7`. As a result, `theano` uses the `betas` variable values from the previous call to our function as the `betas` variable values that it updates in the current call. This ensures that we are continuously updating our `betas` through each call to our function, and over time converging to their true values.\n",
    "\n",
    "Now, we'll dive into some code that uses this function to learn the true values for our `betas` vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual beta_0: 2\n",
      "Actual beta_1: 4\n",
      "Actual beta_2: 4\n",
      "Actual beta_3: 2\n",
      "\n",
      "\n",
      "Initial theano value for beta_0: 0.4116343412290435\n",
      "Initial theano value for beta_1: 0.7835213540150907\n",
      "Initial theano value for beta_2: 0.07552026109669352\n",
      "Initial theano value for beta_3: 0.8763165213674217\n",
      "\n",
      "\n",
      "Initial theano value for beta_0: 1.9999999999818232\n",
      "Initial theano value for beta_1: 3.9999999999392837\n",
      "Initial theano value for beta_2: 3.999999999937256\n",
      "Initial theano value for beta_3: 1.9999999999666893\n"
     ]
    }
   ],
   "source": [
    "# Randomly generate our betas and number of observations, used to generate \n",
    "# fake data to fit. We need a minimum of 4 obs. \n",
    "true_betas_array = np.random.randint(2, 10, size=4)\n",
    "n_obs = np.random.randint(9500, 10000) \n",
    "for idx, beta in enumerate(true_betas_array): \n",
    "        print(\"Actual beta_{}: {}\".format(idx, beta))  \n",
    "print ('\\n')\n",
    "\n",
    "# Generate the data that follows a linear relationship specified \n",
    "# by true_beta_0 and true_beta_1.\n",
    "xs, ys = gen_multiple_logistic(true_betas_array, n_obs)\n",
    "\n",
    "theano_linear_graph = get_theano_graph()\n",
    "for _ in range(50000): \n",
    "    yhats, errors = theano_linear_graph(xs, ys)\n",
    "shared_variables = theano_linear_graph.get_shared()[0].get_value()\n",
    "for idx, beta in enumerate(shared_variables): \n",
    "    print(\"Initial theano value for beta_{}: {}\".format(idx, beta[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we can see, this implementation is pretty similar to our implementation using `numpy`. The biggest difference is that we call our function `theano_linear_graph` at each iteration of our loop, as opposed to having the entire looping process defined within a function. Given that our `theano_linear_graph` is defined to perform only **a single** iteration of our gradient descent procedure, this makes sense. \n",
    "\n",
    "Overall, we see that we can solve our linear regression problem using this computational graph that we defined with `theano`. Now, we'll code this up using `tensorflow`."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [vision]",
   "language": "python",
   "name": "Python [vision]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
