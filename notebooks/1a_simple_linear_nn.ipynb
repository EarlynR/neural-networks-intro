{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression: Neural Net Style\n",
    "\n",
    "We're going to begin our journey into neural networks by starting with simple linear regression, but add a little twist. We're going to fit our simple linear regression and learn its coefficient as if it was a neural network, using gradient descent. We'll use both forward propagation and backward propagation (these terms should become clearer soon), which will help us to (hopefully) transition into talking about neural networks more seamlessly. \n",
    "\n",
    "## Standard Simple Linear Regression\n",
    "\n",
    "Recall that simple linear regression can be denoted with the following formula: \n",
    "\n",
    "<img src=\"imgs/equations/simp_linear.png\" width=110 \\>\n",
    "\n",
    "where \n",
    "<img src=\"imgs/variables/x1.png\" width=20 style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" \\> is a column vector of individual observations. Before solving for our \n",
    "<img src=\"imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> \n",
    "and \n",
    "<img src=\"imgs/variables/beta1.png\" width=20 style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" \\>, we'll need to define an error metric. We'll use **squared error**, as is common to use in linear regression. Thus, our error for an *individual observation* is given by the following formula: \n",
    "\n",
    "<img src=\"imgs/equations/ind_squared_error.png\" width=115 \\>\n",
    "\n",
    "where \n",
    "<img src=\"imgs/variables/yi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=14 \\> \n",
    "is our true value, and \n",
    "<img src=\"imgs/variables/yhati.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=13 \\> \n",
    "is our predicted value. Given this error for an individual observation, the average error (**mean squared error**) across *all observations* is: \n",
    "\n",
    "<img src=\"imgs/equations/agg_squared_error.png\" width=150 \\>\n",
    "\n",
    "Typically, we'll solve linear regression (without regularization) by minimizing this mean squared error, and by doing so we'll obtain the closed form solution for our coefficients: \n",
    "\n",
    "<img src=\"imgs/equations/beta1_soln_simp_linear.png\" width=160 \\>  \n",
    "<img src=\"imgs/equations/beta0_soln_simp_linear.png\" width=120 \\>\n",
    "\n",
    "It turns out that we could also solve our linear regression problem using gradient descent...\n",
    "\n",
    "## Simple Linear Regression using Gradient Descent\n",
    "\n",
    "### Simple Linear Regression in a Computational Graph\n",
    "\n",
    "Before solving our linear regression problem through gradient descent, it'll be useful to view it through a computational graph. This will be our first step towards talking about neural networks, as we'll visualize those in a similar manner. Using a computational graph, we can visualize our simple linear regression problem as follows: \n",
    "\n",
    "<img src=\"imgs/custom/simp_linear_comp_graph.png\" align=\"center\" width=\"400\"\\>\n",
    "\n",
    "Here, we're using blocks (**nodes**) to denote each of the parts of our linear regression equation in our computational graph, which is read from left to right. \n",
    "<img src=\"imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, \n",
    "<img src=\"imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> , and \n",
    "<img src=\"imgs/variables/x1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> \n",
    "each get their own node. Then, we have a multiplication node to denote that \n",
    "<img src=\"imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> and \n",
    "<img src=\"imgs/variables/x1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> \n",
    "are multiplied together. Finally, the summation node denotes that the product of \n",
    "<img src=\"imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>  and \n",
    "<img src=\"imgs/variables/x1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>  is summed with \n",
    "<img src=\"imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> to obtain \n",
    "<img src=\"imgs/variables/y.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=14 \\> . \n",
    "We'll now fit this depiction into a solution using gradient descent. \n",
    "\n",
    "### Solving Simple Linear Regression via Gradient Descent \n",
    "\n",
    "Solving our linear regression problem using gradient descent will involve several steps: \n",
    "\n",
    "1. Randomly initialize values for our coefficients, \n",
    "<img src=\"imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> and\n",
    "<img src=\"imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>    \n",
    "2. While we haven't met some stopping condition:  \n",
    " A. Calculate our predicted outcomes, \n",
    "<img src=\"imgs/variables/yhat.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>. \n",
    "<img src=\"imgs/variables/yhat.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\> \n",
    "will be the the result of plugging in our current values for \n",
    "<img src=\"imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>\n",
    "and \n",
    "<img src=\"imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, as well as our values for \n",
    "<img src=\"imgs/variables/x1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, into our simple linear regression equation (<img src=\"imgs/equations/simp_linear.png\" width=100 style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" \\>). \n",
    "This is the **forward propagation** step.    \n",
    " B. For each of our observations, calculate our error using the true values\n",
    "<img src=\"imgs/variables/yi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=14 \\>, our predicted values \n",
    "<img src=\"imgs/variables/yhati.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>, and our error formula: \n",
    "<img src=\"imgs/equations/ind_squared_error.png\" width=110 \\>\n",
    " C. For each of our observations, calculate the gradient of the error with respect to each one of our coefficients, and then update them accordingly (**backward propagation**).  \n",
    " \n",
    "We'll formalize this more in the next notebook, but for now let's walk through 2A and 2C in more detail, specifically because the terms **forward propagation** and **backward propagation** are key terms when working with neural networks. \n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "In forward propagation, we are going to read our computational graph from left to right, using the calculations it gives to compute our output (which for us is \n",
    "<img src=\"imgs/variables/yhat.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>). That is, we'll **propagate our inputs forward** through our computational graph in order to obtain our output. \n",
    "\n",
    "<img src=\"imgs/custom/simp_linear_comp_graph_forprop.png\" width=400\\>\n",
    "\n",
    "### Backward Propagation\n",
    "\n",
    "In backward propagation, we'll read our computational graph from right to left, but with a set of inputs that is a function of our error. In between forward and backward propagation, we'll calculate our error for each observation (step 2B above). Then, we'll use the mean of the gradient of each of those errors with respect to each of our coefficients to update each coefficient. In the end, we'll want the following two quantities for each observation: \n",
    "\n",
    "<img src=\"imgs/derivatives/ei_beta0.png\" width=30\\>\n",
    "<img src=\"imgs/derivatives/ei_beta1.png\" width=30\\>\n",
    "\n",
    "To obtain these quantities, we'll use the chain rule: \n",
    "\n",
    "<img src=\"imgs/derivatives/ei_beta0_chain.png\" width=120\\>\n",
    "<img src=\"imgs/derivatives/ei_beta1_chain.png\" width=110\\>\n",
    "\n",
    "Visually, this step would look something like the following: \n",
    "\n",
    "<img src=\"imgs/custom/simp_linear_comp_graph_backprop.png\" width=400\\>\n",
    "\n",
    "In the next notebook, we'll formalize our gradient descent procedure, actually walk through calculating those derivatives above, and then code it up using `numpy`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
