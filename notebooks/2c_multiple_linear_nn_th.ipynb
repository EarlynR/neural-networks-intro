{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression using `theano`\n",
    "\n",
    "Instead of using `numpy`, we'll now walk through using gradient descent to solve our multiple linear regression problem with `theano`. In using `theano`, we'll actually build up a computational graph in code, and then let `theano` handle the differentiation for us. While we'll still have to specify what quantity to take the derivative of, and with respect to what parameter(s), this automatic differentiation will still save us the trouble of taking any derivatives by hand. This feature will become invaluable as we move towards neural networks with hundreds of thousands of parameters. \n",
    "\n",
    "## Computational Graphs for Multiple Linear Regression \n",
    "\n",
    "As we'll be coding up our forward and backward propagation steps with `theano`, let's keep their visuals around as a reference: \n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "<img src=\"imgs/custom/mult_linear_comp_graph_condensed_forprop.png\" width=300\\>\n",
    "\n",
    "### Backward Propagation\n",
    "\n",
    "<img src=\"imgs/custom/mult_linear_comp_graph_condensed_backprop.png\" width=400\\>\n",
    "\n",
    "### Building a computational graph with `theano`\n",
    "\n",
    "As mentioned above, the biggest difference between our `numpy` implementation and our `theano` implementation of our multiple linear regression problem is that we'll be able to use the automatic differentiation that `theano` offers. \n",
    "\n",
    "Let's dive in and see what that looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "from datasets.general import gen_multiple_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_theano_graph(): \n",
    "    learning_rate = 0.1\n",
    "    # 1. Define placeholder matrices for inputs. \n",
    "    xs, ys = T.dmatrices('xs', 'ys') # returns one or more matrices of type `float64`\n",
    "    # 2. Define randomly initialized floats for our betas. \n",
    "    betas = theano.shared(np.random.random(size=(4, 1)), name='betas')\n",
    "\n",
    "    # 3. Define the equation that generates predictions. \n",
    "    yhats = T.dot(xs, betas)\n",
    "    # 4. Define the equation that generates our errors. \n",
    "    es = ys - yhats\n",
    "    # 5. Define the aggregate cost (mean of squared errors)\n",
    "    E = (0.5 * (es ** 2)).mean()\n",
    "    # 6. Take advantage of `theanos` automatic differentiation, and use the derivatives\n",
    "    #    to perform the update step. \n",
    "    d_betas = T.grad(E, betas)\n",
    "    updates = [(betas, betas - learning_rate * d_betas)]\n",
    "    # 7. Define a function that we can feed inputs to, obtain outputs from, and \n",
    "    #    perform updates on our coefficients / train them. \n",
    "    train = theano.function(inputs=[xs, ys], outputs=[es, yhats], \n",
    "                            updates=updates)\n",
    "    \n",
    "    for idx, beta in enumerate(betas.get_value()): \n",
    "        print(\"Initial theano value for beta_{}: {}\".format(idx, beta[0]))\n",
    "    print('\\n')\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in our solution for simple linear regression, our `get_theano_graph` returns a `theano.function` object that performs one iteration of our gradient descent procedure. We've named this `theano.function` object `train`, and it performs a forward propagation step (**forward pass**) as well as a backward propagation step (**backward pass**). It does this by traversing the computational graph from left to right, and then right to left (as depicted in the visualizations above). From a high level, the steps are the exact same as they were in our solution for simple linear regression - steps `1-5` define the forward pass, step `6` defines the backward pass, and step `7` [generates a callable function](http://deeplearning.net/software/theano/library/compile/function.html#module-theano.compile.function) that we can later use to perform one iteration of our gradient descent procedure. The only real difference from our solution for simple linear regression is that we have specified our coefficients as a vector (via the `betas` shared variable), rather than individually.  \n",
    "\n",
    "We are again able to take advantage of the automatic differentiation that `theano` offers. In step `6`, we simply use `T.grad`, passing as the first argument the quantity to take the derivative of, and as the second what to take the derivative with respect to (which here is our vector of `betas`). Note here that in order to take advantage of the automatic differentiation, we have to take the mean of the squared-error and then differentiate that, as opposed to differentiating the individual errors as we did with the `numpy` implementation.  The `theano.grad` function expects a scalar input, and mathematically it's all the same, since the derivative of a sum is equal to the sum of the individual derivatives. \n",
    "\n",
    "The callable function generated in step `7` takes inputs as specified via the `inputs` argument, runs them through the graph, and produces returns the outputs specified by the `outputs` argument. This function has side effects that are specified by the `updates` argument, namely that we update the values of each of the betas in our `betas` shared vector. This is how we ultimately learn the values for our coefficients. \n",
    "\n",
    "It's worth noting that we defined our `betas` variable as a [shared variable](http://deeplearning.net/software/theano/tutorial/examples.html#using-shared-variables), which tells `theano` that it should **share** the values held in this variable across calls to the function that we generate in step `7`. Ultimately, this means that `theano` uses the `betas` variable values from the previous call to our function as the `betas` variable values that it updates in the current call. This ensures that we are continuously updating our `betas` through each call to our function, and over time converging to their true values.\n",
    "\n",
    "Now, we'll write some code that actually uses this function to learn the true values for our `betas` vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual beta_0: 5\n",
      "Actual beta_1: 3\n",
      "Actual beta_2: 4\n",
      "Actual beta_3: 8\n",
      "\n",
      "\n",
      "Initial theano value for beta_0: 0.273091158309001\n",
      "Initial theano value for beta_1: 0.8001281871509008\n",
      "Initial theano value for beta_2: 0.3455487120015168\n",
      "Initial theano value for beta_3: 0.3172451756249267\n",
      "\n",
      "\n",
      "Initial theano value for beta_0: 4.999999999999939\n",
      "Initial theano value for beta_1: 3.000000000000046\n",
      "Initial theano value for beta_2: 4.000000000000176\n",
      "Initial theano value for beta_3: 7.999999999999797\n"
     ]
    }
   ],
   "source": [
    "# Randomly generate our betas and number of observations, used to generate \n",
    "# fake data to fit. We need a minimum of 4 obs. \n",
    "true_betas_array = np.random.randint(2, 10, size=4)\n",
    "n_obs = np.random.randint(4, 10) \n",
    "for idx, beta in enumerate(true_betas_array): \n",
    "        print(\"Actual beta_{}: {}\".format(idx, beta))  \n",
    "print ('\\n')\n",
    "\n",
    "# Generate the theano graph. This is in a function so that we can run this cell multiple times\n",
    "# and obtain different randomly generated values for `betas`. \n",
    "theano_linear_graph = get_theano_graph()\n",
    "\n",
    "# Generate the data that follows a linear relationship specified by `true_betas_array`.\n",
    "xs, ys = gen_multiple_linear(true_betas_array, n_obs)\n",
    "\n",
    "# Perform iterations (forward & backward prop.) over the theano graph\n",
    "for _ in range(50000): \n",
    "    yhats, errors = theano_linear_graph(xs, ys)\n",
    "shared_variables = theano_linear_graph.get_shared()[0].get_value()\n",
    "for idx, beta in enumerate(shared_variables): \n",
    "    print(\"Initial theano value for beta_{}: {}\".format(idx, beta[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this implementation is pretty similar to our implementation using `numpy`. The biggest difference is that we call our function `theano_linear_graph` at each iteration of our loop, as opposed to having the entire looping process defined within a function. This makes sense, though, given that our `theano_linear_graph` is defined to perform only **a single** iteration of our gradient descent procedure. \n",
    "\n",
    "Overall, we see that we can solve our linear regression problem using this computational graph that we defined with `theano`, and that just as with our `numpy` implementation, we obtain the expected coefficients. \n",
    "\n",
    "Now, we'll code this up using `tensorflow`..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
