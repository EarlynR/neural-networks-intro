{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multiple Linear Regression: Neural Net Style\n",
    "\n",
    "We'll move on from simple linear regression and look at multiple linear regression, again from a neural network perspective. We'll solve our multiple linear regression problem via forward and backward propagation using gradient descent, similar to how we would train a neural network. \n",
    "\n",
    "## Standard Multiple Linear Regression\n",
    "\n",
    "Typically, we'll denote multiple linear regression with the following mathematical formula: \n",
    "\n",
    "<img src=\"../imgs/equations/mult_linear.png\" width=140 \\>\n",
    "\n",
    "where each \n",
    "<img src=\"../imgs/variables/xj.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=22 \\> \n",
    "is an individual column vector from the full X matrix that contains \n",
    "<img src=\"../imgs/variables/p.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=14 \\>\n",
    "features along its columns and observations along the rows. To keep our problem relatively simple, we're going to assume that we only have three features. Given that, our mathematical formula above turns into: \n",
    "\n",
    "<img src=\"../imgs/equations/mult_linear_3_feats.png\" width=140 \\>\n",
    "\n",
    "In this case, we'll need to solve for \n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, \n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\> , \n",
    "<img src=\"../imgs/variables/beta2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=19 \\> , \n",
    "and\n",
    "<img src=\"../imgs/variables/beta3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=18 \\>. Just as with simple linear regression, we'll need to define an error metric before doing so. We'll again use **squared error**, since it's a common metric to use in linear regression. This means that our error for an *individual observation* is given by: \n",
    "\n",
    "<img src=\"../imgs/equations/ind_squared_error.png\" width=115 \\>\n",
    "\n",
    "where \n",
    "<img src=\"../imgs/variables/yi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=14 \\> \n",
    "is our true value, and \n",
    "<img src=\"../imgs/variables/yhati.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=13 \\> \n",
    "is our predicted value. The average error (**mean squared error**) across *all observations* is then: \n",
    "\n",
    "<img src=\"../imgs/equations/agg_squared_error.png\" width=150 \\>\n",
    "\n",
    "If we were to solve this multiple linear regression problem (without regularization) by minimizing this squared-error using derivatives and matrix algebra, we'd obtain the following closed form solution for our\n",
    "<img src=\"../imgs/variables/beta.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=14 \\>\n",
    "vector: \n",
    "\n",
    "<img src=\"../imgs/equations/beta_soln_mult_linear.png\" width=150 \\>\n",
    "\n",
    "Just as with simple linear regression, though, we can (and will) also solve our multiple linear regression problem using gradient descent...\n",
    "\n",
    "## Multiple Linear Regression using Gradient Descent\n",
    "\n",
    "### Multiple Linear Regression in a Computational Graph\n",
    "\n",
    "Before diving in to actually solve our multiple linear regression problem using gradient descent, we'll view it as a computational graph. This will help us to better visualize and understand neural networks when we later move to those. Visualized as a computational graph, our multiple linear regression looks as follows: \n",
    "\n",
    "<img src=\"../imgs/custom/mult_linear_comp_graph.png\" align=\"center\" width=\"400\"\\>\n",
    "\n",
    "Just as with our computational graph for simple linear regression, we're using the colored blocks (**nodes**) to denote each part of our multiple linear regression equation. Reading the graph from left to right, we can see that each of the coefficients \n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\>, \n",
    "<img src=\"../imgs/variables/beta2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=19 \\>, \n",
    "and \n",
    "<img src=\"../imgs/variables/beta3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=18 \\> \n",
    "is multiplied with it's corresponding column vector (\n",
    "<img src=\"../imgs/variables/x1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=18 \\>, \n",
    "<img src=\"../imgs/variables/x2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=18 \\>, \n",
    "or\n",
    "<img src=\"../imgs/variables/x3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=19 \\>\n",
    "), \n",
    "as denoted by the blue multiplication nodes. Finally, the results of those multiplications are summed together, along with \n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>. This is denoted by the blue sum node, and its output is our final output, \n",
    "<img src=\"../imgs/variables/y.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=15 \\>. \n",
    "\n",
    "Before we move on to looking at the forward and backward propagation steps, we're going to simplify our visual a little bit (this will help as we build towards more involved graphs). Instead of denoting the multiplication of our coefficients and inputs with two separate nodes, we're simply going to denote this multiplication by placing the coefficient above the link connecting the input vectors (\n",
    "<img src=\"../imgs/variables/x1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=18 \\>, \n",
    "<img src=\"../imgs/variables/x2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=18 \\>, \n",
    "and\n",
    "<img src=\"../imgs/variables/x3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=19 \\>\n",
    ") with the summation node. For \n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, we'll simply get rid of its node, but leave it in the same place. This will help denote that it is not associated with an input vector (or rather, that it's associated with an input vector of 1's). Our simplified visual looks as follows: \n",
    "\n",
    "<img src=\"../imgs/custom/mult_linear_comp_graph_condensed.png\" align=\"center\" width=\"300\"\\>\n",
    "\n",
    "Next, we'll move on to solving our multiple linear regression problem with gradient descent. Here, the depiction of our computational graph above will come in handy. \n",
    "\n",
    "### Solving Multiple Linear Regression via Gradient Descent \n",
    "\n",
    "Solving our multiple linear regression problem with gradient descent is going to take the same form as solving our simple linear regression problem with gradient descent, but with two additional coefficients to take into account (<img src=\"../imgs/variables/beta2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=19 \\> and\n",
    "<img src=\"../imgs/variables/beta3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=18 \\>). To solve, we'll take the following steps: \n",
    "\n",
    "1. Randomly initialize values for our coefficients: \n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, \n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\> , \n",
    "<img src=\"../imgs/variables/beta2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=19 \\> , and\n",
    "<img src=\"../imgs/variables/beta3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=18 \\>. \n",
    "2. While we haven't met some stopping condition:  \n",
    " A. Calculate our predicted outcomes, \n",
    "<img src=\"../imgs/variables/yhat.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>. \n",
    "<img src=\"../imgs/variables/yhat.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\> \n",
    "will be the the result of plugging in our current values for \n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, \n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\> , \n",
    "<img src=\"../imgs/variables/beta2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=19 \\> , and\n",
    "<img src=\"../imgs/variables/beta3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=18 \\>  as well as our values for \n",
    "<img src=\"../imgs/variables/x1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, \n",
    "<img src=\"../imgs/variables/x2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, and\n",
    "<img src=\"../imgs/variables/x3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>\n",
    "into our multiple linear regression equation (<img src=\"../imgs/equations/mult_linear_3_feats.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=140 \\>). \n",
    "This is the **forward propagation** step.    \n",
    " B. For each of our observations, calculate our error using the true values\n",
    "<img src=\"../imgs/variables/yi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=14 \\>, our predicted values \n",
    "<img src=\"../imgs/variables/yhati.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>, and our error formula: \n",
    "<img src=\"../imgs/equations/ind_squared_error.png\" width=110 \\>\n",
    " C. For each of our observations, calculate the gradient of the error with respect to each one of our coefficients, and then update them accordingly (**backward propagation**).  \n",
    "\n",
    "We'll formalize this entire procedure in the next notebook (especially step 2C), but for now let's highlight steps 2A and 2C. \n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "With forward propagation, we'll simply read our computational graph from left to right in order to compute our output (\n",
    "<img src=\"../imgs/variables/yhat.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>).\n",
    "\n",
    "<img src=\"../imgs/custom/mult_linear_comp_graph_condensed_forprop.png\" width=300\\>\n",
    "\n",
    "### Backward Propagation\n",
    "\n",
    "With backward propagation, we'll read our computational graph in reverse, from right to left. The inputs that we start out with will be our errors that we calculate for each observation (step 2B above). In backward propagation, we'll calculate the gradient of each of those errors with respect to each of our coefficients, and update our coefficients with the mean of those gradients. Ultimately, we'll want the following quantities for each observation: \n",
    "\n",
    "<img src=\"../imgs/derivatives/ei_beta0.png\" width=30\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta1.png\" width=30\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta2.png\" width=30\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta3.png\" width=30\\>\n",
    "\n",
    "We'll obtain these quantities in the same way that we did when working through our simple linear regression problem, using the chain rule: \n",
    "\n",
    "<img src=\"../imgs/derivatives/ei_beta0_chain.png\" width=120\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta1_chain.png\" width=110\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta2_chain.png\" width=120\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta3_chain.png\" width=110\\>\n",
    "\n",
    "Visualizing this through our computational graph would look as follows: \n",
    "\n",
    "<img src=\"../imgs/custom/mult_linear_comp_graph_condensed_backprop.png\" width=400\\>\n",
    "\n",
    "In the next notebook, we'll work through the calculations of those derivatives, and actually code this up using `numpy`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
