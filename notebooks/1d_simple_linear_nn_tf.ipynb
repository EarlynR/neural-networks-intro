{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression using `tensorflow`\n",
    "\n",
    "Now that we've coded up our linear regression solution by building a computational graph with `theano`, we'll do the same with `tensorflow`. From a high level, we'll do exactly what we did before - we'll build a computational graph, and then take advantage of the automatic differentiation that `tensorflow` offers. \n",
    "\n",
    "## Computational Graphs for Simple Linear Regression \n",
    "\n",
    "As a reference, the computational graphs that we used to visualize the forward and backward propagation steps in solving our simple linear regression problem with gradient descent are as follows: \n",
    "\n",
    "### Forward Propagation \n",
    "\n",
    "<img src=\"imgs/custom/simp_linear_comp_graph_forprop.png\" width=400\\>\n",
    "\n",
    "### Backward Propagation \n",
    "\n",
    "<img src=\"imgs/custom/simp_linear_comp_graph_backprop.png\" width=400\\>\n",
    "\n",
    "### Building a computational graph with `tensorflow`\n",
    "\n",
    "In the same manner as we coded up our computational graph using `theano`, let's look at how to code this up one piece at a time with `tensorflow`, discussing along the way how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets.general import gen_simple_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tensorflow_graph(): \n",
    "    # 1. Define placeholder matrices for inputs.\n",
    "    xs = tf.placeholder(tf.float32, name='xs') \n",
    "    ys = tf.placeholder(tf.float32, name='ys')\n",
    "    # 2. Define randomly initialized floats for beta_0 and beta_1. \n",
    "    beta_0 = tf.Variable(np.random.random(), name='beta_0')\n",
    "    beta_1 = tf.Variable(np.random.random(), name='beta_1')\n",
    "\n",
    "    # 3. Define the equation that generates predictions.\n",
    "    yhats = beta_0 + xs * beta_1\n",
    "    # 4. Define the equation that generates our errors. \n",
    "    es = ys - yhats\n",
    "    # 5. Define the aggregate cost (mean of squared errors)\n",
    "    E = (0.5 * (es ** 2))\n",
    "    # 6. Take advantage of `tensorflows` automatic differentiation. \n",
    "    d_beta_0, d_beta_1 = tf.gradients(E, [beta_0, beta_1])\n",
    "\n",
    "    # 7. Perform the update step, and use `assign` to actually update the value. \n",
    "    new_beta_0 = beta_0 - 0.1 * tf.reduce_mean(d_beta_0)\n",
    "    new_beta_1 = beta_1 - 0.1 * tf.reduce_mean(d_beta_1)\n",
    "    beta_0_update = tf.assign(beta_0, new_beta_0)\n",
    "    beta_1_update = tf.assign(beta_1, new_beta_1)\n",
    "    \n",
    "    # Unfortunately, to have this working as a function, we have to return all of these \n",
    "    # individual pieces. \n",
    "    return beta_0, beta_1, beta_0_update, beta_1_update, xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a high level, `get_tensorflow_graph` returns back the steps necessary to perform one iteration of our gradient descent procedure. It's a little different from `get_theano_graph`, though, in that it returns back individual pieces, rather than a callable function. In general, the steps still more or less line up - steps `1-5` define the **forward pass**, step `6` defines the **backward pass**, and step `7` sets us up to be able to perform an iteration of the gradient descent procedure and update our coefficients accordingly.\n",
    "\n",
    "Let's walk through each of the individual pieces being returned...\n",
    "\n",
    "1. `beta_0` and `beta_1` are returned simply so we can look at their initial values before any training begins. When run in a session below, these will hold the random values that `beta_0` and `beta_1` were initialized with in step `2`. Note that both `beta_0` and `beta_1` were intialized using as [Variable objects](https://www.tensorflow.org/versions/r0.9/get_started/basic_usage.html#variables), which is what allows their values to be shared and updated across iterations. \n",
    "\n",
    "2. `beta_0_update` and `beta_1_update` hold the computational graph that will be run in order to perform an iteration of the gradient descent procedure. When run in a session below, any steps that are necessary to perform the coefficient update specified by `beta_0_update` and `beta_1_update` will be run, which in effect is every step that is part of the forward and backward propagation. The [use of assign](https://www.tensorflow.org/versions/r0.9/api_docs/python/state_ops.html#Variable) in step `7` is what allows us to actually update the values for `beta_0` and `beta_1`, whereas `beta_0_update` and `beta_1_update` are what we use to refer to these updates and later tell `tensorflow` to perform them. \n",
    "\n",
    "3. Finally, `xs` and `ys` are placeholders for our data, and are returned so that we can tell `tensorflow` exactly what part of our graph our real data should line up with. \n",
    "\n",
    "Now, let's write the code that runs a `tensorflow` session to learn the values for `beta_0` and `beta_1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual beta_0: 9\n",
      "Actual beta_1: 5\n",
      "\n",
      "Initial tensorflow value for beta_0: 0.7671453952789307\n",
      "Initial tensorflow for beta_1: 0.41680485010147095\n",
      "\n",
      "Final tensorflow value for beta_0: 8.999984741210938\n",
      "Final tensorflow for beta_1: 5.000018119812012\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly generate a beta_0, beta_1, and number of observations, used to generate \n",
    "# fake data to fit. We need a minimum of 2 obs. \n",
    "true_beta_0, true_beta_1, n_obs = np.random.randint(2, 10, size=3) \n",
    "print('Actual beta_0: {}'.format(true_beta_0))\n",
    "print('Actual beta_1: {}\\n'.format(true_beta_1))\n",
    "\n",
    "# Generate the tensorflow graph. This is in a function so that we can run this cell multiple \n",
    "# times and obtain different randomly generated values for `beta_0` and `beta_1`. \n",
    "beta_0, beta_1, beta_0_update, beta_1_update, xs, ys = get_tensorflow_graph()\n",
    "\n",
    "# Generate the data that follows a linear relationship specified \n",
    "# by true_beta_0 and true_beta_1.\n",
    "x, y = gen_simple_linear(true_beta_0, true_beta_1, n_obs)\n",
    "\n",
    "# Define the initialization operation. \n",
    "init = tf.initialize_variables([beta_0, beta_1])\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init) # Perform the actual initialization operation. \n",
    "    \n",
    "    # Obtain the initial values of beta_0, beta_1 to print\n",
    "    init_beta_0, init_beta_1 = sess.run([beta_0, beta_1])\n",
    "    print(\"Initial tensorflow value for beta_0: {}\".format(init_beta_0))\n",
    "    print(\"Initial tensorflow for beta_1: {}\\n\".format(init_beta_1))\n",
    "    \n",
    "    # Perform iterations (forward & backward prop.) over the tensorflow graph\n",
    "    for step in range(5000):\n",
    "        beta_0, beta_1 = sess.run([beta_0_update, beta_1_update], feed_dict={xs : x, ys : y}) \n",
    "    print(\"Final tensorflow value for beta_0: {}\".format(beta_0))\n",
    "    print(\"Final tensorflow for beta_1: {}\\n\".format(beta_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparing our `tensorflow` implementation to either our `numpy` or `theano` implementation, the biggest difference we can see is that our `tensorflow` implementation is run via a `session` object. A [session object](https://www.tensorflow.org/versions/r0.9/api_docs/python/client.html#session-management) allows us to encapsulate all of the calculations and implementation details of our graph (or any  graph) into a single environment. This turns out to be  useful when we want to run multiple graphs, each of which might have their own specialized environment.  \n",
    "\n",
    "After creating the `session` object, our first step is to [initialize any variables](https://www.tensorflow.org/versions/r0.9/how_tos/variables/index.html#initialization) that we're going to need to use in the graph. This is all of the `tf.Variable` objects that we have created in our graph, which for us is just `beta_0` and `beta_1`. If we have so many variables that we don't want to type them all out, there is also a `tf.initialize_all_variables` function that we can call.\n",
    "\n",
    "Once we have created a `session` object and initialized all of our variables, we can run parts of our graph by calling `session.run` and passing in the piece of the graph that we want calculated. For example, when we want to view the initial values for `beta_0` and `beta_1`, we run: \n",
    "\n",
    "```\n",
    "sess.run([beta_0, beta_1])\n",
    "``` \n",
    "\n",
    "If we want to view the values for `beta_0` and `beta_1` **after** they have been updated once, we run the following: \n",
    "\n",
    "```\n",
    "sess.run([beta_0_update, beta_1_update], feed_dict={xs : x, ys : y})\n",
    "```\n",
    "\n",
    "When these `run` commands are executed, any steps necessary to compute what is asked for will be run. For the first call above, where we ask for `beta_0` and `beta_1`, this is only the initialization step (`2` in our first code cell). For the second call, where we ask for `beta_0_update` and `beta_1_update`, this is every step in the graph. This includes everything in the forward propagation steps (`1-5`), everything in the backward propagation step (`6`), as well as the update itself (`7`). Note, too, that `xs` and `ys` are needed in the second call, and we pass these in via the `feed_dict` argument. For the `feed_dict` argument, the keys are the variable names referring to the placeholder objects, and the values are the data that will be used for those placeholders.\n",
    "\n",
    "Finally, if we run that second call in a loop (as we do), we see that we can solve our linear regression using this graph built in `tensorflow` and obtain the coefficient values that we expect.  \n",
    "\n",
    "Now, we'll move on to coding this up with `keras`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
