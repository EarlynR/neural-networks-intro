{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression using Gradient Descent\n",
    "\n",
    "In our last notebook, we walked through a higher level overview of using gradient descent to solve our multiple linear regression problem. Now, we'll work through the gradient desecent procedure in more detail, and code it up in `numpy`. \n",
    "\n",
    "## Gradient Descent for Multiple Linear Regression\n",
    "\n",
    "With gradient descent, we'll do the following: \n",
    "\n",
    "1. Randomly initialize values for our coefficients: \n",
    "<img src=\"imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, \n",
    "<img src=\"imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\> , \n",
    "<img src=\"imgs/variables/beta2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=19 \\> , and\n",
    "<img src=\"imgs/variables/beta3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=18 \\>. \n",
    "\n",
    "2. While we haven't met some stopping condition:   \n",
    " A. Calculate our predicted outcomes, \n",
    "<img src=\"imgs/variables/yhat.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>.  \n",
    " B. Calculate the error for each observation using the true values\n",
    "<img src=\"imgs/variables/yi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=14 \\>, \n",
    "our predicted values \n",
    "<img src=\"imgs/variables/yhati.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>, \n",
    "and our error formula: \n",
    "<img src=\"imgs/equations/ind_squared_error.png\" width=110 \\>      \n",
    " C. For each observation, calculate the gradient of the error with respect to each one of our coefficients (\n",
    "<img src=\"imgs/derivatives/ei_beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>, \n",
    "<img src=\"imgs/derivatives/ei_beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>, \n",
    "<img src=\"imgs/derivatives/ei_beta2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>, \n",
    "<img src=\"imgs/derivatives/ei_beta3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>\n",
    "), and then use the average across observations to update the coefficients: \n",
    "<img src=\"imgs/updates/beta0_simp_linear_update.png\" width=150 \\>\n",
    "<img src=\"imgs/updates/beta1_simp_linear_update.png\" width=150 \\>\n",
    "<img src=\"imgs/updates/beta2_simp_linear_update.png\" width=150 \\>\n",
    "<img src=\"imgs/updates/beta3_simp_linear_update.png\" width=150 \\>\n",
    "\n",
    "To calculate the gradients for each observation in 2C, we'll use the chain rule that we looked at last notebook: \n",
    "\n",
    "<img src=\"imgs/derivatives/ei_beta0_chain.png\" width=120\\>\n",
    "<img src=\"imgs/derivatives/ei_beta1_chain.png\" width=120\\>\n",
    "<img src=\"imgs/derivatives/ei_beta2_chain.png\" width=120\\>\n",
    "<img src=\"imgs/derivatives/ei_beta3_chain.png\" width=120\\>\n",
    "\n",
    "We can break these equations down into calculating each of the individual pieces - \n",
    "<img src=\"imgs/derivatives/ei_yi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"imgs/derivatives/yhati_beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"imgs/derivatives/yhati_beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>\n",
    "<img src=\"imgs/derivatives/yhati_beta2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"imgs/derivatives/yhati_beta3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=35\\>. We can calculate those as follows: \n",
    "\n",
    "<img src=\"imgs/derivatives/ei_yi_soln.png\" width=275 \\>\n",
    "<img src=\"imgs/derivatives/yhati_beta0_soln.png\" width=75 \\>\n",
    "<img src=\"imgs/derivatives/yhati_beta1_soln.png\" width=90 \\>\n",
    "<img src=\"imgs/derivatives/yhati_beta2_soln.png\" width=90 \\>\n",
    "<img src=\"imgs/derivatives/yhati_beta3_soln.png\" width=90 \\>\n",
    "\n",
    "If we plug these back into the original equations, we can obtain our full updates for step 2C: \n",
    "\n",
    "<img src=\"imgs/derivatives/ei_beta0_chain_soln.png\" width=350\\>\n",
    "<img src=\"imgs/derivatives/ei_beta1_chain_soln.png\" width=290\\>\n",
    "<img src=\"imgs/derivatives/ei_beta2_chain_soln.png\" width=290\\>\n",
    "<img src=\"imgs/derivatives/ei_beta3_chain_soln.png\" width=290\\>\n",
    "\n",
    "Now, let's code this up! \n",
    "\n",
    "## Multiple Linear Regression using Gradient Descent with `numpy`\n",
    "\n",
    "To demonstrate using gradient descent to solve our multiple linear regression problem, we'll use the `gen_multiple_linear` function from the `datasets/general.py` file to generate some toy data that follows a multivariate linear relationship (with three variables). We'll simply pass in a `1d numpy array` of betas as well as a number of observations to the function, and it will return back to us inputs and outputs (our `xs` and `ys`). With that `xs` and `ys` matrix, we'll learn the values for our coefficients (\n",
    "<img src=\"imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, \n",
    "<img src=\"imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\> , \n",
    "<img src=\"imgs/variables/beta2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=19 \\> , and\n",
    "<img src=\"imgs/variables/beta3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=18 \\>\n",
    ") using gradient descent, and we'll also double check our results using the `LinearRegression` model from `sklearn`. \n",
    "\n",
    "The biggest difference between our implementation of simple linear regression using gradient descent and multiple linear regression using gradient descent will be that we will move to exclusively using vectors and matrices. Instead of having our beta coefficients as `beta_0` and `beta_1`, we'll simply have a beta vector that will hold each of our betas (\n",
    "<img src=\"imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, \n",
    "<img src=\"imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\> , \n",
    "<img src=\"imgs/variables/beta2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=19 \\> , and\n",
    "<img src=\"imgs/variables/beta3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=18 \\>\n",
    "). It's worth nothing that this means that the first column of the `xs` matrix returned will be a vector of 1's that will be lined up with our \n",
    "<img src=\"imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>. Other than this move to vectors and matrices, the code will look pretty much the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datasets.general import gen_multiple_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn_w_gradient_descent(xs, ys): \n",
    "    learning_rate = 0.1\n",
    "    # Step 1 - randomly initialize values for our coefficients, and print their values.  \n",
    "    betas_array = np.random.random(size=4)\n",
    "    for idx, beta in enumerate(betas_array): \n",
    "        print(\"Initial gradient descent beta_{}: {}\".format(idx, beta))    \n",
    "    print('\\n')\n",
    "        \n",
    "    for _ in range(50000): \n",
    "        # Step 2A - calculate our predicted outcomes. \n",
    "        yhats = xs.dot(betas_array)\n",
    "        yhats = yhats.reshape(len(yhats), 1) # Force `yhats` to have a second dimension for later calculations.\n",
    "        \n",
    "        # Step 2B - calculate our errors. \n",
    "        es = (ys - yhats)\n",
    "        \n",
    "        # Step 2C - calculate the gradient of the error with respect to the coefficients, \n",
    "        # and use it to update the coefficients. \n",
    "        # es = es.reshape(len(es), 1) # Force `es` to have second dimension of 1 for later calculations. \n",
    "        d_betas_array = -es * xs\n",
    "        betas_array -= learning_rate * d_betas_array.mean(axis=0)\n",
    "        \n",
    "    for idx, beta in enumerate(betas_array): \n",
    "        print(\"Final gradient descent beta_{}: {}\".format(idx, beta))   \n",
    "    print('\\n')\n",
    "          \n",
    "def learn_w_standard_linear_regression(xs, ys): \n",
    "    \n",
    "    linear_model = LinearRegression(fit_intercept=False)\n",
    "    linear_model.fit(xs, ys) \n",
    "    \n",
    "    for idx, beta in enumerate(linear_model.coef_[0]): \n",
    "        print(\"Sklearn linear regression beta_{}: {}\".format(idx, beta))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual beta_0: 7\n",
      "Actual beta_1: 8\n",
      "Actual beta_2: 2\n",
      "Actual beta_3: 3\n",
      "\n",
      "\n",
      "Initial gradient descent beta_0: 0.4140000739788944\n",
      "Initial gradient descent beta_1: 0.5489335388529588\n",
      "Initial gradient descent beta_2: 0.042475990151779164\n",
      "Initial gradient descent beta_3: 0.2616382915198292\n",
      "\n",
      "\n",
      "Final gradient descent beta_0: 7.000000000000087\n",
      "Final gradient descent beta_1: 7.999999999999846\n",
      "Final gradient descent beta_2: 1.9999999999999707\n",
      "Final gradient descent beta_3: 2.9999999999999503\n",
      "\n",
      "\n",
      "Sklearn linear regression beta_0: 7.0000000000000036\n",
      "Sklearn linear regression beta_1: 8.000000000000004\n",
      "Sklearn linear regression beta_2: 2.0000000000000018\n",
      "Sklearn linear regression beta_3: 2.999999999999992\n"
     ]
    }
   ],
   "source": [
    "# Randomly generate our betas and number of observations, used to generate \n",
    "# fake data to fit. We need a minimum of 4 obs. \n",
    "true_betas_array = np.random.randint(2, 10, size=4)\n",
    "n_obs = np.random.randint(4, 10) \n",
    "for idx, beta in enumerate(true_betas_array): \n",
    "        print(\"Actual beta_{}: {}\".format(idx, beta))  \n",
    "print ('\\n')\n",
    "        \n",
    "# Generate the data that follows a linear relationship specified \n",
    "# by true_beta_0 and true_beta_1.\n",
    "xs, ys = gen_multiple_linear(true_betas_array, n_obs)\n",
    "\n",
    "# Learn the coefficients using gradient descent and `LinearRegression` from sklearn. \n",
    "learn_w_gradient_descent(xs, ys)\n",
    "linear_model = learn_w_standard_linear_regression(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with simple linear regression, we see that we can in fact solve multiple linear regression using gradient descent. We can even run it multiple times, and see that each time, our gradient descent procedure is able to learn the right coefficient values for each of our betas. This will help set the stage for understanding complicated neural network architectures that we'll look at in subsequent notebooks. Most (if not all) neural networks can be viewed as having a **forward** and **backward** propagation step where we can use some flavor of gradient descent to update and learn our coefficients (often called weights in neural network land).\n",
    "\n",
    "We'll now move on to coding this up using `theano`, a python library that allows us to define computational graphs and benefit from automatic differentiation. Libraries like this will be extremely useful when building incredibly complicated neural networks for which it is difficult and time consuming to derive the update rules by hand. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
