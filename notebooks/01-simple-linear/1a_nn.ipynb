{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression: Neural Net Style\n",
    "\n",
    "We're going to begin our journey into neural networks by starting with simple linear regression, but add a little twist. We're going to fit our simple linear regression and learn its coefficients as if it was a neural network, using gradient descent. We'll use both forward propagation and backward propagation (these terms should become clearer soon), which will help us to (hopefully) transition into talking about neural networks more seamlessly. \n",
    "\n",
    "## Standard Simple Linear Regression\n",
    "\n",
    "Recall that simple linear regression can be denoted with the following formula: \n",
    "\n",
    "<img src=\"../imgs/equations/simp_linear.png\" width=110 \\>\n",
    "\n",
    "where \n",
    "<img src=\"../imgs/variables/x1.png\" width=20 style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" \\> is a column vector of individual observations. Before solving for \n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> \n",
    "and \n",
    "<img src=\"../imgs/variables/beta1.png\" width=20 style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" \\>, we'll need to define an error metric. We'll use **squared error**, as is common to use in linear regression. Thus, the error for an *individual observation* is given by the following formula: \n",
    "\n",
    "<img src=\"../imgs/equations/ind_squared_error.png\" width=115 \\>\n",
    "\n",
    "where \n",
    "<img src=\"../imgs/variables/yi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=17 \\> \n",
    "is the true value, and \n",
    "<img src=\"../imgs/variables/yhati.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=16 \\> \n",
    "is our predicted value. Given this error for an individual observation, the average error (**mean squared error**) across *all observations* is: \n",
    "\n",
    "<img src=\"../imgs/equations/agg_squared_error.png\" width=150 \\>\n",
    "\n",
    "Typically, we'll [solve simple linear regression](https://isites.harvard.edu/fs/docs/icb.topic515975.files/OLSDerivation.pdf) (without regularization) by minimizing this mean squared error, and by doing so we'll obtain the closed form solution for\n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> \n",
    "and \n",
    "<img src=\"../imgs/variables/beta1.png\" width=22 style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" \\>: \n",
    "\n",
    "<img src=\"../imgs/equations/beta1_soln_simp_linear.png\" width=160 \\>  \n",
    "<img src=\"../imgs/equations/beta0_soln_simp_linear.png\" width=120 \\>\n",
    "\n",
    "It turns out that we could also solve our linear regression problem using gradient descent...\n",
    "\n",
    "## Simple Linear Regression using Gradient Descent\n",
    "\n",
    "### Simple Linear Regression in a Computational Graph\n",
    "\n",
    "Before solving our simple linear regression problem using gradient descent, it'll be useful to view it as a computational graph. This will be our first step towards talking about neural networks, as we'll visualize those in a similar manner. Using a computational graph, we can visualize our simple linear regression problem as follows: \n",
    "\n",
    "<img src=\"../imgs/custom/simp_linear_comp_graph.png\" align=\"center\" width=\"400\"\\>\n",
    "\n",
    "Here, we're using blocks (**nodes**) to denote each of the parts of our simple linear regression equation in a computational graph, which is read from left to right. To begin, \n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, \n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\> , and \n",
    "<img src=\"../imgs/variables/x1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> \n",
    "each get their own node. Then, we have a multiplication node to denote that \n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> and \n",
    "<img src=\"../imgs/variables/x1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> \n",
    "are multiplied together. Finally, the summation node denotes that the product of \n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\>  and \n",
    "<img src=\"../imgs/variables/x1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>  is summed with \n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> to obtain \n",
    "<img src=\"../imgs/variables/y.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=14 \\> . \n",
    "We'll now fit this depiction into a solution using gradient descent. \n",
    "\n",
    "### Solving Simple Linear Regression via Gradient Descent \n",
    "\n",
    "Solving our simple linear regression problem using gradient descent will involve several steps: \n",
    "\n",
    "1. Randomly initialize values for our coefficients, \n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> and\n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\>    \n",
    "2. While we haven't met some stopping condition:  \n",
    " A. Calculate our predicted outcomes, \n",
    "<img src=\"../imgs/variables/yhat.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=14 \\>, using our simple linear regression equation\n",
    "(<img src=\"../imgs/equations/simp_linear.png\" width=100 style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" \\>). \n",
    "This is the **forward propagation** step.    \n",
    " B. For each observation, calculate the error using the true values\n",
    "<img src=\"../imgs/variables/yi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=17 \\>, our predicted values \n",
    "<img src=\"../imgs/variables/yhati.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=16 \\>, and the error formula: \n",
    "<img src=\"../imgs/equations/ind_squared_error.png\" width=110 \\>\n",
    " C. For each observation, calculate the gradient of the error with respect to each one of our coefficients, and then update them accordingly (**backward propagation**).  \n",
    " \n",
    "We'll formalize this procedure a little more in the next notebook, but for now let's focus on steps 2A and 2C. This is in part because the terms **forward propagation** and **backward propagation** are terms that we'll use often when working with neural networks. \n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "In forward propagation, we are simply going to read the computational graph from **left to right** (as we did before), using the calculations it gives to compute our output (which in this case is \n",
    "<img src=\"../imgs/variables/yhat.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>).\n",
    "\n",
    "<img src=\"../imgs/custom/simp_linear_comp_graph_forprop.png\" width=400\\>\n",
    "\n",
    "### Backward Propagation\n",
    "\n",
    "In backward propagation, we'll read our computational graph from **right to left**, where the errors are the inputs. In between forward and backward propagation, we'll calculate the error for each observation (step 2B above). Next, we'll calculate the gradient of each of those errors with respect to each of the coefficients. Finally, we'll use the mean of those gradients to update the coefficients. To get the mean, we'll want the following two quantities for each observation: \n",
    "\n",
    "<img src=\"../imgs/derivatives/ei_beta0.png\" width=30\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta1.png\" width=30\\>\n",
    "\n",
    "To obtain these quantities, we'll use the chain rule: \n",
    "\n",
    "<img src=\"../imgs/derivatives/ei_beta0_chain.png\" width=120\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta1_chain.png\" width=110\\>\n",
    "\n",
    "Visually, this step would look something like the following: \n",
    "\n",
    "<img src=\"../imgs/custom/simp_linear_comp_graph_backprop.png\" width=400\\>\n",
    "\n",
    "In the next notebook, we'll formalize our gradient descent procedure, actually walk through calculating those derivatives, and then code it up using `numpy`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
