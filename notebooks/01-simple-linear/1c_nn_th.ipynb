{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression using `theano`\n",
    "\n",
    "To solve our simple linear regression problem with gradient descent using `numpy`, we coded each individual equation, which included the calculation of the derivatives (which we calculated first by hand). With `theano`, we'll avoid having to code each individual equation by building a computational graph, and letting `theano` handle the differentiation for us. \n",
    "\n",
    "With the automatic differentiation that `theano` offers, we'll still have to specify some details on how to take the derivatives, but we will no longer have to calculate any derivatives by hand! This will be incredibly beneficial when we move to neural networks that have hundreds of thousands of parameters (imagine calculating the derivatives for all those by hand). \n",
    "\n",
    "## Computational Graphs for Simple Linear Regression \n",
    "\n",
    "Recall the computational graphs that we used to visualize the forward and backward propagation steps in solving our simple linear regression problem with gradient descent: \n",
    "\n",
    "### Forward Propagation \n",
    "\n",
    "<img src=\"../imgs/custom/simp_linear_comp_graph_forprop.png\" width=400\\>\n",
    "\n",
    "### Backward Propagation \n",
    "\n",
    "<img src=\"../imgs/custom/simp_linear_comp_graph_backprop.png\" width=400\\>\n",
    "\n",
    "### Building a computational graph with `theano`\n",
    "\n",
    "For the most part, building up simple linear regression as a computational graph using `theano` is going to look pretty similar to our `numpy` implementation, but with different syntax. As mentioned above, the biggest benefit we'll get from building this graph will be automatic differentiation. We'll simply be able to tell `theano` that we want the derivative of some quantity with respect to some coefficient/parameter, and it'll calculate the derivative for us. \n",
    "\n",
    "Let's look at how to code this up one piece at a time, discussing along the way how it works... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "from datasets.general import gen_simple_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_theano_graph(): \n",
    "    learning_rate = 0.1\n",
    "    # 1. Define placeholder matrices for inputs. \n",
    "    xs, ys = T.dmatrices('xs', 'ys') # returns one or more matrices of type `float64`\n",
    "    # 2. Define randomly initialized floats for beta_0 and beta_1. \n",
    "    beta_0 = theano.shared(np.random.random(), name='beta_0')\n",
    "    beta_1 = theano.shared(np.random.random(), name='beta_1')\n",
    "\n",
    "    # 3. Define the equation that generates predictions. \n",
    "    yhats = beta_0 + beta_1 * xs\n",
    "    # 4. Define the equation that generates our errors. \n",
    "    es = ys - yhats\n",
    "    # 5. Define the aggregate cost (mean of squared errors)\n",
    "    E = (0.5 * (es ** 2)).mean()\n",
    "    # 6. Take advantage of `theanos` automatic differentiation. \n",
    "    d_beta_0, d_beta_1 = T.grad(E, [beta_0, beta_1])\n",
    "    # 7. Define a function that we can feed inputs to, obtain outputs from, and \n",
    "    #    perform updates on our coefficients / train them. \n",
    "    train = theano.function(inputs=[xs, ys], outputs=[es, yhats], \n",
    "                            updates=((beta_0, beta_0 - learning_rate * d_beta_0), \n",
    "                                     (beta_1, beta_1 - learning_rate * d_beta_1)))\n",
    "    \n",
    "    print(\"Initial theano value for beta_0: {}\".format(beta_0.get_value()))\n",
    "    print(\"Initial theano value for beta_1: {}\\n\".format(beta_1.get_value()))\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a high level, `get_theano_graph` returns a `theano.function` object that performs one iteration of our gradient descent procedure. It performs forward propagation (steps `1-5`) as well as backward propagation (step `6`). The final step, `7`, simply [generates a callable function](http://deeplearning.net/software/theano/library/compile/function.html#module-theano.compile.function) that we can later use to perform one iteration of our gradient descent procedure. \n",
    "\n",
    "The magic of the automatic differentiation that `theano` offers is what we see in step `6`. To take advantage of it, we have to use `T.grad`, passing as the first argument the quantity to take the derivative of and as the second what to take the derivative with respect to. Note here that we have to take the mean of the squared-error and then differentiate that, as opposed to differentiating the individual errors as we did with the `numpy` implementation.  The `theano.grad` function expects a scalar input, and mathematically it's all the same, since the derivative of a sum is equal to the sum of the individual derivatives. \n",
    "\n",
    "The callable function generated in step `7` can take inputs (via the `inputs` argument), run them through the graph, and produce outputs (via the `outputs` argument). This function has side effects that are specified by the `updates` argument, namely that we update the values of `beta_0` and `beta_1`. Ultimately, these updates are how we learn the values for these coefficients. \n",
    "\n",
    "Finally, it's worth noting that `beta_0` and `beta_1` are defined as [shared variables](http://deeplearning.net/software/theano/tutorial/examples.html#using-shared-variables) (step `2`). When we define them in this way, `theano` knows that it should **share** the values of these variables across calls to the function that is generated in step `7`. That is, `theano` should use the values of `beta_0` and `beta_1` from the previous call to the function as the values that it uses when it updates them in the current call. This ensures that we are continuously updating `beta_0` and `beta_1` through each call to the function, and over time converging to their true values.\n",
    "\n",
    "Now, we'll write some code that actually uses this function to learn the true values for a given `beta_0` and `beta_1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual beta_0: 6\n",
      "Actual beta_1: 2\n",
      "\n",
      "Initial theano value for beta_0: 0.4900850048961227\n",
      "Initial theano value for beta_1: 0.9519716594042242\n",
      "\n",
      "Final theano value for beta_0: 5.9999999999998614\n",
      "Final theano value for beta_1: 2.0000000000002487\n"
     ]
    }
   ],
   "source": [
    "# Randomly generate a beta_0, beta_1, and number of observations, used to generate \n",
    "# fake data to fit. We need a minimum of 2 obs. \n",
    "true_beta_0, true_beta_1, n_obs = np.random.randint(2, 10, size=3) \n",
    "print('Actual beta_0: {}'.format(true_beta_0))\n",
    "print('Actual beta_1: {}\\n'.format(true_beta_1))\n",
    "\n",
    "# Generate the theano graph. This is in a function so that we can run this cell multiple times\n",
    "# and obtain different randomly generated values for `beta_0` and `beta_1`. \n",
    "theano_linear_graph = get_theano_graph()\n",
    "\n",
    "# Generate the data that follows a linear relationship specified \n",
    "# by true_beta_0 and true_beta_1.\n",
    "xs, ys = gen_simple_linear(true_beta_0, true_beta_1, n_obs)\n",
    "\n",
    "# Perform iterations (forward & backward prop.) over the theano graph\n",
    "for _ in range(5000): \n",
    "    yhats, errors = theano_linear_graph(xs, ys)\n",
    "shared_variables = theano_linear_graph.get_shared()\n",
    "print(\"Final theano value for beta_0: {}\".format(shared_variables[0].get_value()))\n",
    "print(\"Final theano value for beta_1: {}\".format(shared_variables[1].get_value()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we obtain the callable function from `get_theano_graph`, there is only one real difference from our implementation using `numpy`. Rather than performing our looping (e.g. iterations of gradient descent) within the function that we generated, we perform the looping external to the function, calling the function `theano_linear_graph` each time through the loop. This isn't really a notable difference, though, and makes sense given that the function generated from `get_theano_graph` is defined to perform **one** iteration of gradient descent. Note above, too, that `xs` and `ys` are passed as inputs to this callable function, just as the function signature specifies. \n",
    "\n",
    "In the end, we see that we can solve our linear regression problem using this computational graph that we defined in `theano`, and that just as with the `numpy` implementation, we obtain the expected coefficients. \n",
    "\n",
    "Next, we'll code this up using `tensorflow`... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
