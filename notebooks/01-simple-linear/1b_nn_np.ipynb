{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression using Gradient Descent\n",
    "\n",
    "Last notebook, we looked at a kind of high level overview of solving our linear regression problem using gradient descent. Now, we'll formalize that, actually calculate the derivatives that we need to implement it, and then use `numpy` to do so. \n",
    "\n",
    "## Gradient Descent for Simple Linear Regression\n",
    "\n",
    "Formally, with gradient descent we will do the following: \n",
    "\n",
    "1. Randomly initialize values for our coefficients, \n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> and\n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>   \n",
    "\n",
    "2. While we haven't met some stopping condition:   \n",
    " A. Calculate our predicted outcomes, \n",
    "<img src=\"../imgs/variables/yhat.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>.  \n",
    " B. Calculate the error for each observation using the true values\n",
    "<img src=\"../imgs/variables/yi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=14 \\>, \n",
    "our predicted values \n",
    "<img src=\"../imgs/variables/yhati.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>, \n",
    "and our error formula: \n",
    "<img src=\"../imgs/equations/ind_squared_error.png\" width=110 \\>      \n",
    " C. For each observation, calculate the gradient of the error with respect to each one of our coefficients (\n",
    "<img src=\"../imgs/derivatives/ei_beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>, \n",
    "<img src=\"../imgs/derivatives/ei_beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>\n",
    "), and then use the average across observations to update the coefficients: \n",
    "<img src=\"../imgs/updates/beta0_simp_linear_update.png\" width=150 \\>\n",
    "<img src=\"../imgs/updates/beta1_simp_linear_update.png\" width=150 \\>\n",
    "\n",
    "Note that we are subtracting off the gradient in step 2C because the gradient gives us the direction of steepest ascent (and we want to take the direction of steepest descent to minimize our error). Note also that \n",
    "<img src=\"../imgs/variables/alpha.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=15\\> \n",
    "in step 2C is simply the learning rate (e.g. how much of the coefficient updates actually get applied). Now, let's actually calculate those derivatives. Recall that we'll use the chain rule to calculate the updates that we need for step 2C: \n",
    "\n",
    "<img src=\"../imgs/derivatives/ei_beta0_chain.png\" width=120\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta1_chain.png\" width=110\\>\n",
    "\n",
    "To calculate these, we'll need three quantities - \n",
    "<img src=\"../imgs/derivatives/ei_yi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"../imgs/derivatives/yhati_beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"../imgs/derivatives/yhati_beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>. We can calculate these as follows: \n",
    "\n",
    "<img src=\"../imgs/derivatives/ei_yi_soln.png\" width=275 \\>\n",
    "<img src=\"../imgs/derivatives/yhati_beta0_soln.png\" width=75 \\>\n",
    "<img src=\"../imgs/derivatives/yhati_beta1_soln.png\" width=90 \\>\n",
    "\n",
    "We can then plug these in to obtain our updates for step 2C: \n",
    "\n",
    "<img src=\"../imgs/derivatives/ei_beta0_chain_soln.png\" width=350\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta1_chain_soln.png\" width=290\\>\n",
    "\n",
    "Note that the update for \n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>\n",
    "is actually just the negative of our errors calculated in step 2B! \n",
    "\n",
    "## Simple Linear Regression using Gradient Descent with `numpy`\n",
    "\n",
    "Now, we'll code this up using `numpy`! We'll use a function from the `datasets/general.py` file to generate fake data that follows an inputted linear relationship (we'll specify the <img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> \n",
    "and \n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> to generate data to follow that relationship), and then learn the coefficients using gradient descent as specified above. We'll also double check our results with the `LinearRegression` model from `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datasets.general import gen_simple_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn_w_gradient_descent(xs, ys): \n",
    "    learning_rate = 0.1\n",
    "    # Step 1 - randomly initialize values for our coefficients. \n",
    "    beta_0, beta_1 = np.random.randint(1, 10, size=2)\n",
    "    print(\"Initial gradient descent beta_0: {}\".format(beta_0))              \n",
    "    print(\"Initial gradient descent beta_1: {}\".format(beta_1))\n",
    "\n",
    "    for _ in range(5000): \n",
    "        # Step 2A - calculate our predicted outcomes. \n",
    "        yhats = beta_0 + beta_1 * xs\n",
    "        \n",
    "        # Step 2B - calculate our errors. \n",
    "        es = (ys - yhats)\n",
    "        \n",
    "        # Step 2C - calculate the gradient of the error with respect to the coefficients, \n",
    "        # and use it to update the coefficients. \n",
    "        d_beta_0 = -es\n",
    "        d_beta_1 = -es * xs\n",
    "        beta_0 -= learning_rate * d_beta_0.mean()\n",
    "        beta_1 -= learning_rate * d_beta_1.mean()\n",
    "          \n",
    "    print(\"Final gradient descent beta_0: {}\".format(beta_0))              \n",
    "    print(\"Final gradient descent beta_1: {}\\n\".format(beta_1))\n",
    "    \n",
    "def learn_w_standard_linear_regression(xs, ys): \n",
    "    \n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(xs, ys)\n",
    "    \n",
    "    print(\"Sklearn linear regression beta_0: {}\".format(linear_model.intercept_[0]))\n",
    "    print(\"Sklearn linear regression beta_1: {}\".format(linear_model.coef_[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual beta_0: 5\n",
      "Actual beta_1: 8\n",
      "\n",
      "Initial gradient descent beta_0: 3\n",
      "Initial gradient descent beta_1: 9\n",
      "Final gradient descent beta_0: 4.999999999999938\n",
      "Final gradient descent beta_1: 8.000000000000142\n",
      "\n",
      "Sklearn linear regression beta_0: 5.0\n",
      "Sklearn linear regression beta_1: 8.000000000000002\n"
     ]
    }
   ],
   "source": [
    "# Randomly generate a beta_0, beta_1, and number of observations, used to generate \n",
    "# fake data to fit. We need a minimum of 2 obs. \n",
    "true_beta_0, true_beta_1, n_obs = np.random.randint(2, 10, size=3) \n",
    "print('Actual beta_0: {}'.format(true_beta_0))\n",
    "print('Actual beta_1: {}\\n'.format(true_beta_1))\n",
    "\n",
    "# Generate the data that follows a linear relationship specified \n",
    "# by true_beta_0 and true_beta_1.\n",
    "xs, ys = gen_simple_linear(true_beta_0, true_beta_1, n_obs)\n",
    "\n",
    "# Learn the coefficients using gradient descent and `LinearRegression` from sklearn. \n",
    "learn_w_gradient_descent(xs, ys)\n",
    "learn_w_standard_linear_regression(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above demonstrates that we can in fact solve linear regression using gradient descent, and that we do obtain the coefficients that we expect. We can even run it multiple times, and see that each time, our gradient descent procedure is able to learn the right coefficient values for \n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> \n",
    "and \n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>. While this in itself isn't terribly useful, it'll help set the stage for understanding complicated neural network architectures that we'll look at in subsequent notebooks. At the end of the day, most (if not all) neural networks can be viewed as having a **forward** and **backward** propagation step where we can use some flavor of gradient descent to update and learn our coefficients (often called weights in neural network land).\n",
    "\n",
    "We'll now move on to coding this up using `theano`, a python library that allows us to define computational graphs and benefit from automatic differentiation. Libraries like this will be extremely useful when building incredibly complicated neural networks for which it is difficult and time consuming to derive the update rules by hand. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
