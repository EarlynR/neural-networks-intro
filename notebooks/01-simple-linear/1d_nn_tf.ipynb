{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression using `tensorflow`\n",
    "\n",
    "Now that we've coded up our simple linear regression solution by building a computational graph with `theano`, we'll do the same with `tensorflow`. Once we build this computational graph, we'll take advantage of the automatic differentiation that `tensorflow` offers. \n",
    "\n",
    "## Computational Graphs for Simple Linear Regression \n",
    "\n",
    "As a reference, the computational graphs that we used to visualize the forward and backward propagation steps in solving our simple linear regression problem with gradient descent are as follows: \n",
    "\n",
    "### Forward Propagation \n",
    "\n",
    "<img src=\"../imgs/custom/simp_linear_comp_graph_forprop.png\" width=400\\>\n",
    "\n",
    "### Backward Propagation \n",
    "\n",
    "<img src=\"../imgs/custom/simp_linear_comp_graph_backprop.png\" width=400\\>\n",
    "\n",
    "### Building a computational graph with `tensorflow`\n",
    "\n",
    "In the same manner as we coded up our computational graph using `theano`, let's look at how to code this up one piece at a time with `tensorflow`, discussing along the way how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets.general import gen_simple_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tensorflow_graph(): \n",
    "    learning_rate = 0.1\n",
    "    # 1. Define placeholder matrices for inputs.\n",
    "    xs = tf.placeholder(tf.float32, name='xs') \n",
    "    ys = tf.placeholder(tf.float32, name='ys')\n",
    "    # 2. Define randomly initialized floats for beta_0 and beta_1. \n",
    "    beta_0 = tf.Variable(np.random.random(), name='beta_0')\n",
    "    beta_1 = tf.Variable(np.random.random(), name='beta_1')\n",
    "\n",
    "    # 3. Define the equation that generates predictions.\n",
    "    yhats = beta_0 + xs * beta_1\n",
    "    # 4. Define the equation that generates our errors. \n",
    "    es = ys - yhats\n",
    "    # 5. Define the aggregate cost (mean of squared errors)\n",
    "    E = (0.5 * (es ** 2))\n",
    "    # 6. Take advantage of `tensorflows` automatic differentiation. \n",
    "    d_beta_0, d_beta_1 = tf.gradients(E, [beta_0, beta_1])\n",
    "\n",
    "    # 7. Perform the update step, and use `assign` to actually update the value. \n",
    "    new_beta_0 = beta_0 - learning_rate * tf.reduce_mean(d_beta_0)\n",
    "    new_beta_1 = beta_1 - learning_rate * tf.reduce_mean(d_beta_1)\n",
    "    beta_0_update = tf.assign(beta_0, new_beta_0)\n",
    "    beta_1_update = tf.assign(beta_1, new_beta_1)\n",
    "    \n",
    "    # Unfortunately, to have this working as a function, we have to return all of these \n",
    "    # individual pieces. \n",
    "    return beta_0, beta_1, beta_0_update, beta_1_update, xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a high level, `get_tensorflow_graph` returns back the steps necessary to perform one iteration of our gradient descent procedure. It's a little different from `get_theano_graph` (notebook `1c`), though, in that it returns back individual pieces rather than a callable function. In general, the steps still more or less line up - steps `1-5` define the **forward pass**, step `6` defines the **backward pass**, and step `7` sets us up to be able to perform an iteration of the gradient descent procedure and update our coefficients accordingly.\n",
    "\n",
    "Let's walk through each of the individual pieces being returned...\n",
    "\n",
    "1. `beta_0` and `beta_1` are returned so that we can look at their initial values before any training begins. When run in a session before any updates are performed, these will hold the random values that `beta_0` and `beta_1` were initialized with in step `2`. Note that both `beta_0` and `beta_1` are initialized as [Variable objects](https://www.tensorflow.org/versions/r0.9/get_started/basic_usage.html#variables), which is what allows their values to be shared and updated across iterations.  \n",
    "\n",
    "2. `beta_0_update` and `beta_1_update` hold the computational graph that will be run in order to perform an iteration of the gradient descent procedure. When run in a session below, any steps that are necessary to perform the coefficient update specified by `beta_0_update` and `beta_1_update` will be run, which in effect is every step that is part of the forward and backward propagation. The [use of assign](https://www.tensorflow.org/versions/r0.9/api_docs/python/state_ops.html#Variable) in step `7` is what allows us to actually update the values for `beta_0` and `beta_1`, whereas `beta_0_update` and `beta_1_update` are what we use to refer to these updates and later tell `tensorflow` to perform them.   \n",
    "\n",
    "3. Finally, `xs` and `ys` are placeholders for our data, and are returned so that we can tell `tensorflow` exactly what part of the graph our real data should line up with. \n",
    "\n",
    "Now, let's write the code that runs a `tensorflow` session to learn the values for `beta_0` and `beta_1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual beta_0: 4\n",
      "Actual beta_1: 7\n",
      "\n",
      "Initial tensorflow value for beta_0: 0.2582991421222687\n",
      "Initial tensorflow for beta_1: 0.5820571780204773\n",
      "\n",
      "Final tensorflow value for beta_0: 4.000013828277588\n",
      "Final tensorflow for beta_1: 6.999972820281982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly generate a beta_0, beta_1, and number of observations, used to generate \n",
    "# fake data to fit. We need a minimum of 2 obs. \n",
    "true_beta_0, true_beta_1, n_obs = np.random.randint(2, 10, size=3) \n",
    "print('Actual beta_0: {}'.format(true_beta_0))\n",
    "print('Actual beta_1: {}\\n'.format(true_beta_1))\n",
    "\n",
    "# Generate the tensorflow graph. This is in a function so that we can run this cell multiple \n",
    "# times and obtain different randomly generated values for `beta_0` and `beta_1`. \n",
    "beta_0, beta_1, beta_0_update, beta_1_update, xs, ys = get_tensorflow_graph()\n",
    "\n",
    "# Generate the data that follows a linear relationship specified \n",
    "# by true_beta_0 and true_beta_1.\n",
    "x, y = gen_simple_linear(true_beta_0, true_beta_1, n_obs)\n",
    "\n",
    "# Define the initialization operation. \n",
    "init = tf.initialize_variables([beta_0, beta_1])\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init) # Perform the actual initialization operation. \n",
    "    \n",
    "    # Obtain the initial values of beta_0, beta_1 to print\n",
    "    init_beta_0, init_beta_1 = sess.run([beta_0, beta_1])\n",
    "    print(\"Initial tensorflow value for beta_0: {}\".format(init_beta_0))\n",
    "    print(\"Initial tensorflow for beta_1: {}\\n\".format(init_beta_1))\n",
    "    \n",
    "    # Perform iterations (forward & backward prop.) over the tensorflow graph\n",
    "    for step in range(5000):\n",
    "        beta_0, beta_1 = sess.run([beta_0_update, beta_1_update], feed_dict={xs : x, ys : y}) \n",
    "    print(\"Final tensorflow value for beta_0: {}\".format(beta_0))\n",
    "    print(\"Final tensorflow for beta_1: {}\\n\".format(beta_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparing our `tensorflow` implementation to either our `numpy` or `theano` implementation, the biggest difference we can see is that our `tensorflow` implementation is run via a `Session` object. A [Session object](https://www.tensorflow.org/versions/r0.9/api_docs/python/client.html#session-management) allows us to encapsulate all of the calculations and implementation details of our graph (or any  graph) into a single environment. This turns out to be  useful when we want to run multiple graphs, each of which might have their own specialized environment.  \n",
    "\n",
    "After creating the `Session` object, our first step is to [initialize any variables](https://www.tensorflow.org/versions/r0.9/how_tos/variables/index.html#initialization) that will be used in the graph. For us, this is just `beta_0` and `beta_1`. Once we have created a `Session` object and initialized all of our variables, we can run pieces of our graph or ask for the values of variables by passing them into `Session.run`. For example, any time that we want to view the values for `beta_0` and `beta_1`, we run: \n",
    "\n",
    "```\n",
    "Session.run([beta_0, beta_1])\n",
    "``` \n",
    "\n",
    "If we run this **before** any iterations of gradient descent, then we'll simply get back the initial values that were given to `beta_0` and `beta_1`. If we want to run one iteration of our gradient descent procedure, we run the following: \n",
    "\n",
    "```\n",
    "Session.run([beta_0_update, beta_1_update], feed_dict={xs : x, ys : y})\n",
    "```\n",
    "\n",
    "When this piece (or any piece) of the computational graph is passed into `Session.run`, any steps necessary to compute what is asked for will be run (for any `tf.Variable` the value is simply returned). For `beta_0_update` and `beta_1_update`, this is every step in the graph, including the forward propagation (steps `1-5`), the backward propagation (step `6`), and the update itself (`7`). Note that `xs` and `ys` are needed here, and we pass these in via the `feed_dict` argument. For the `feed_dict` argument, the keys are the variable names referring to the placeholder objects in the graph, and the values are the data that will be used for those placeholders. After `beta_0_update` and `beta_1_update` are passed through `Session.run`, the values for `beta_0` and `beta_1` will no longer correspond to the values they were initialized with. \n",
    "\n",
    "Finally, if we run `beta_0_update` and `beta_1_update` in a loop (as we do), we see that we can solve our simple linear regression using this graph built in `tensorflow`.  \n",
    "\n",
    "Now, we'll move on to coding this up with `keras`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
